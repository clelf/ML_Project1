{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Additionally added libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "#### Remember to unpack data from .zip folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138.47 ,  51.655,  97.827, ...,   1.24 ,  -2.475, 113.497],\n",
       "       [160.937,  68.768, 103.235, ...,     nan,     nan,  46.226],\n",
       "       [    nan, 162.172, 125.953, ...,     nan,     nan,  44.251],\n",
       "       ...,\n",
       "       [105.457,  60.526,  75.839, ...,     nan,     nan,  41.992],\n",
       "       [ 94.951,  19.362,  68.812, ...,     nan,     nan,   0.   ],\n",
       "       [    nan,  72.756,  70.831, ...,     nan,     nan,   0.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting -999 to NaN values\n",
    "tX[tX==-999] = np.nan\n",
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical exploratory data analysis\n",
    "# row1: mean | row2: variance | row3: standard deviation | row4: minimum value | row5: maximum value | row6: number of NaN's\n",
    "feature_details = np.zeros([6, tX.shape[1]])\n",
    "for i in range(tX.shape[1]):\n",
    "    feature_details[0, i] = np.nanmean(tX[:,i])\n",
    "    feature_details[1, i] = np.nanvar(tX[:,i])\n",
    "    feature_details[2, i] = np.nanstd(tX[:,i])\n",
    "    feature_details[3, i] = np.nanmin(tX[:,i])\n",
    "    feature_details[4, i] = np.nanmax(tX[:,i])\n",
    "    feature_details[5, i] = np.isnan(tX[:,i]).sum()\n",
    "#.... I dont know how to represent this in a nice way without pandas.DataFrame, without doing a lot of coding...\n",
    "#print(feature_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Statistics</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>121.858528</td>\n",
       "      <td>49.239819</td>\n",
       "      <td>81.181982</td>\n",
       "      <td>57.895962</td>\n",
       "      <td>2.403735</td>\n",
       "      <td>371.783360</td>\n",
       "      <td>-0.821688</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>18.917332</td>\n",
       "      <td>158.432217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010119</td>\n",
       "      <td>209.797178</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>84.822105</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.012393</td>\n",
       "      <td>57.679474</td>\n",
       "      <td>-0.011845</td>\n",
       "      <td>-0.001582</td>\n",
       "      <td>73.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>3283.063262</td>\n",
       "      <td>1249.255942</td>\n",
       "      <td>1666.975303</td>\n",
       "      <td>4052.029594</td>\n",
       "      <td>3.035311</td>\n",
       "      <td>158162.573194</td>\n",
       "      <td>12.847474</td>\n",
       "      <td>0.612947</td>\n",
       "      <td>496.106539</td>\n",
       "      <td>13387.851528</td>\n",
       "      <td>...</td>\n",
       "      <td>3.284138</td>\n",
       "      <td>16002.060938</td>\n",
       "      <td>0.955358</td>\n",
       "      <td>3679.887218</td>\n",
       "      <td>3.184583</td>\n",
       "      <td>3.288345</td>\n",
       "      <td>1023.076126</td>\n",
       "      <td>4.127921</td>\n",
       "      <td>3.301261</td>\n",
       "      <td>9607.031571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>57.298021</td>\n",
       "      <td>35.344815</td>\n",
       "      <td>40.828609</td>\n",
       "      <td>63.655554</td>\n",
       "      <td>1.742214</td>\n",
       "      <td>397.696584</td>\n",
       "      <td>3.584337</td>\n",
       "      <td>0.782910</td>\n",
       "      <td>22.273449</td>\n",
       "      <td>115.705884</td>\n",
       "      <td>...</td>\n",
       "      <td>1.812219</td>\n",
       "      <td>126.499253</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>60.662074</td>\n",
       "      <td>1.784540</td>\n",
       "      <td>1.813379</td>\n",
       "      <td>31.985561</td>\n",
       "      <td>2.031729</td>\n",
       "      <td>1.816937</td>\n",
       "      <td>98.015466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.044000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.329000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.602000</td>\n",
       "      <td>-18.066000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.104000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>13.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>-4.499000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1192.026000</td>\n",
       "      <td>690.075000</td>\n",
       "      <td>1349.351000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>8.503000</td>\n",
       "      <td>4974.979000</td>\n",
       "      <td>16.690000</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>1852.462000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2003.976000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1120.573000</td>\n",
       "      <td>4.499000</td>\n",
       "      <td>3.141000</td>\n",
       "      <td>721.456000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>1633.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-NaNs</th>\n",
       "      <td>38114.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99913.000000</td>\n",
       "      <td>99913.000000</td>\n",
       "      <td>99913.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>177457.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0            1            2            3   \\\n",
       "Statistics                                                        \n",
       "Mean          121.858528    49.239819    81.181982    57.895962   \n",
       "Variance     3283.063262  1249.255942  1666.975303  4052.029594   \n",
       "Std            57.298021    35.344815    40.828609    63.655554   \n",
       "min             9.044000     0.000000     6.329000     0.000000   \n",
       "max          1192.026000   690.075000  1349.351000  2834.999000   \n",
       "n-NaNs      38114.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "                       4              5              6         7   \\\n",
       "Statistics                                                          \n",
       "Mean             2.403735     371.783360      -0.821688  2.373100   \n",
       "Variance         3.035311  158162.573194      12.847474  0.612947   \n",
       "Std              1.742214     397.696584       3.584337  0.782910   \n",
       "min              0.000000      13.602000     -18.066000  0.208000   \n",
       "max              8.503000    4974.979000      16.690000  5.684000   \n",
       "n-NaNs      177457.000000  177457.000000  177457.000000  0.000000   \n",
       "\n",
       "                     8             9   ...        20            21        22  \\\n",
       "Statistics                             ...                                     \n",
       "Mean          18.917332    158.432217  ... -0.010119    209.797178  0.979176   \n",
       "Variance     496.106539  13387.851528  ...  3.284138  16002.060938  0.955358   \n",
       "Std           22.273449    115.705884  ...  1.812219    126.499253  0.977424   \n",
       "min            0.000000     46.104000  ... -3.142000     13.678000  0.000000   \n",
       "max         2834.999000   1852.462000  ...  3.142000   2003.976000  3.000000   \n",
       "n-NaNs         0.000000      0.000000  ...  0.000000      0.000000  0.000000   \n",
       "\n",
       "                      23            24            25             26  \\\n",
       "Statistics                                                            \n",
       "Mean           84.822105     -0.003275     -0.012393      57.679474   \n",
       "Variance     3679.887218      3.184583      3.288345    1023.076126   \n",
       "Std            60.662074      1.784540      1.813379      31.985561   \n",
       "min            30.000000     -4.499000     -3.142000      30.000000   \n",
       "max          1120.573000      4.499000      3.141000     721.456000   \n",
       "n-NaNs      99913.000000  99913.000000  99913.000000  177457.000000   \n",
       "\n",
       "                       27             28           29  \n",
       "Statistics                                             \n",
       "Mean            -0.011845      -0.001582    73.064591  \n",
       "Variance         4.127921       3.301261  9607.031571  \n",
       "Std              2.031729       1.816937    98.015466  \n",
       "min             -4.500000      -3.142000     0.000000  \n",
       "max              4.500000       3.142000  1633.433000  \n",
       "n-NaNs      177457.000000  177457.000000     0.000000  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas just to make this numerical EDA nicer to view. \n",
    "# IMPORTANT!!! DO NOT USE IN PROJECT SUBMISSION\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(feature_details)\n",
    "df.index = ['Mean', 'Variance', 'Std', 'min', 'max', 'n-NaNs']\n",
    "df.index.name = 'Statistics'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to standardize dataset - a wide variety of ranges can be observed in the dataset. Would lead to a biased learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store mean and variance\n",
    "feature_mean = feature_details[0, :]\n",
    "feature_std = feature_details[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28991353  0.06833197  0.40768027 ...  0.61614788 -1.36131161\n",
      "   0.4125105 ]\n",
      " [ 0.68202131  0.55250482  0.54013641 ...         nan         nan\n",
      "  -0.27381996]\n",
      " [        nan  3.19515553  1.09655998 ...         nan         nan\n",
      "  -0.29396985]\n",
      " ...\n",
      " [-0.28624947  0.31931645 -0.13086367 ...         nan         nan\n",
      "  -0.31701723]\n",
      " [-0.46960659 -0.84532397 -0.30297338 ...         nan         nan\n",
      "  -0.74543941]\n",
      " [        nan  0.66533608 -0.25352276 ...         nan         nan\n",
      "  -0.74543941]]\n"
     ]
    }
   ],
   "source": [
    "# create standardized dataset\n",
    "#should be put into a method\n",
    "tX_standardized = np.zeros(tX.shape)\n",
    "for i in range(tX_standardized.shape[1]):\n",
    "    tX_standardized[:,i] = (tX[:,i] - feature_mean[i])/feature_std[i]\n",
    "print(tX_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double checking array\n",
    "tX_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ...,  True,  True, False],\n",
       "       [ True, False, False, ...,  True,  True, False],\n",
       "       ...,\n",
       "       [False, False, False, ...,  True,  True, False],\n",
       "       [False, False, False, ...,  True,  True, False],\n",
       "       [ True, False, False, ...,  True,  True, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tX_standardized\n",
    "np.isnan(tX_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving arrays to .py file. Can easily be loaded with np.load('path' + 'filename')\n",
    "####np.save('tX_cleaned', tX)\n",
    "####np.save('tX_standardized', tX_standardized)\n",
    "# Commented this section out to prevent overwriting of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning nan values and replacing them with mean (can also be median or interpolation)\n",
    "from clean_nan import clean_nan\n",
    "tx_ = clean_nan(tX_standardized, feature_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.23032492e-04 -4.68683738e-05 -7.67613927e-05 -5.45887273e-05\n",
      " -1.79758431e-06 -3.50833288e-04  3.51704547e-07 -2.22081221e-06\n",
      " -1.82737099e-05 -1.49871156e-04 -1.64530880e-06  3.29941617e-07\n",
      "  1.14363197e-08 -3.63119276e-05 -7.45562178e-09  4.15404186e-09\n",
      " -4.42101078e-05  1.20479257e-09 -3.52098663e-08 -3.93942645e-05\n",
      "  2.24092356e-08 -1.98464242e-04 -1.26071155e-06 -8.01430558e-05\n",
      "  8.38551156e-09  1.42248815e-08 -5.45339454e-05  1.29077320e-08\n",
      " -7.57439822e-09  4.22163069e-07]\n",
      "0.45263941362763455\n"
     ]
    }
   ],
   "source": [
    "#from least_squares_GD import *\n",
    "from costs import compute_mse\n",
    "from least_squares_GD import *\n",
    "from parameter_tuning import gamma_tuning_SGD\n",
    "\n",
    "# Initialization of the weights\n",
    "initial_w = np.zeros(tx_.shape[1])\n",
    "\n",
    "# Define the parameters necessary for gradient descent: need to tune gamma. \n",
    "# we use the gamma_tuning_SGD for less costly tuning\n",
    "max_iters = 50\n",
    "gamma = gamma_tuning_SGD(tx_, y, initial_w, 1000, max_iters)\n",
    "#gamma=0.0000001\n",
    "\n",
    "#w1, loss1 = least_squares_GD(y, tx_, initial_w, max_iters, gamma)\n",
    "#print(w1)\n",
    "#print(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-07\n"
     ]
    }
   ],
   "source": [
    "print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from least_squares_SGD import *\n",
    "#No need to re-tune gamma BUT NOT SURE\n",
    "#Here batch_size is set at 1\n",
    "\n",
    "\n",
    "# Initialization of the weights BUT we could use w1 as a start\n",
    "initial_w = np.zeros(tx_.shape[1])\n",
    "\n",
    "\n",
    "w2, loss2 = least_squares_SGD(y, tx_, initial_w, 1, max_iters, gamma)\n",
    "#print(w2)\n",
    "#print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares using Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39799914050690927\n",
      "0.37969719657315903\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "from build_polynomial import *\n",
    "from parameter_tuning import degree_tuning_LS\n",
    "\n",
    "degree = degree_tuning_LS(y , tx_)\n",
    "# Initialization of the weights BUT could use w2\n",
    "initial_w = np.zeros(tx_.shape[1]) \n",
    "\n",
    "w3, loss3 = least_squares(y, tx_)\n",
    "_, loss3_ = least_squares(y, poly_x)\n",
    "print(loss3)\n",
    "print(loss3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import *\n",
    "from parameter_tuning import lambda_tuning_ridge\n",
    "\n",
    "# Tuning of lambda\n",
    "lambda_ = lambda_tuning_ridge(y, tx_)\n",
    "print(lambda_)\n",
    "\n",
    "# Computing the loss\n",
    "w4, loss4 = ridge_regression(y, tx_, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.43825356 1.42560418 1.43287537 ... 1.44123761 1.60587793 1.60587793]\n",
      "[inf inf inf ... inf inf inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleme\\Documents\\GitHub\\ML_Project1\\scripts\\logistic_regression.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  loss = calculate_loss_log(y,tx,w)\n",
      "C:\\Users\\cleme\\Documents\\GitHub\\ML_Project1\\scripts\\logistic_regression.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  # store w and loss\n",
      "C:\\Users\\cleme\\Documents\\GitHub\\ML_Project1\\scripts\\logistic_regression.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  # *************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[inf inf inf ... inf inf inf]\n",
      "[-2.30680711e+08 -8.94082150e+07 -1.46980076e+08 -1.04830153e+08\n",
      " -3.58222116e+06 -6.72775948e+08  7.43818015e+05 -4.16125346e+06\n",
      " -3.47926599e+07 -2.87418930e+08 -2.92166043e+06  2.25392959e+05\n",
      " -1.53275750e+05 -6.97026202e+07 -7.31631558e+03  8.98870285e+03\n",
      " -8.45636823e+07  1.98979626e+03 -7.82383388e+04 -7.54698813e+07\n",
      "  2.86705685e+04 -3.80538602e+08 -2.87225519e+06 -1.53485129e+08\n",
      "  1.74434287e+04  2.72568396e+04 -1.04272258e+08  2.42679216e+04\n",
      " -1.39012027e+04  5.92283705e+05]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "\n",
    "# Initialization of the weights BUT could use w2\n",
    "initial_w = w3\n",
    "w5, loss5 = logistic_regression(y, tx_, initial_w, max_iters, gamma)\n",
    "print(w5)\n",
    "print(loss5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
